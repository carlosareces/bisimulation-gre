\section{Evaluation}
\label{sec:evaluation}

In this section we present a quantitative evaluation of the algorithm proposed in this paper. In the GRE area there was a common assumption that there is a gold standard ordering for a given domain~\cite{Dale1995}. However, this assumption has been dropped after empirical studies such as those presented in~\cite{arec2:2008:Areces,viet:gene11}. It has been observed that not only there is no single ordering of properties that covers all human-produced descriptions in a given domain but, in fact, it is not even the case that each speaker consistently uses just one ordering. In this section we show that the non-deterministic algoritm presented in the previous section is able to generate a distribution of REs similar to that observed in corpora, even when no corpus specific for a target object is available. 

Using \puse learned as described in Section~\ref{sec:learning} and running our algorithm 10000 times, we obtain 21 referring expressions for Figure~\ref{GRE3D7-stimulus}. The algorithm generates 8 of the 12 different kinds of REs observed in the 140 ocurrences of the corpora. We also generate other 13 REs for the target, not present in the corpora, but natural sounding as can be observed in Table~\ref{results-algo-fig3} that only represent a 1,64\% of the utterances generated by the algorithm. Hence, 98,36\% of the utterances generated by the algorithm appear in the corpora. In the table we list all the REs found in the corpus for Figure~\ref{GRE3D7-stimulus} and all the RE generated by our algorithm using the learned \puse for the same figure. For each RE, we indicate the number of times it appears in the corpus (\#Cor), the proportion of the corpus it frequency represents (\%Cor), the number of times it is generated by our algorithm (\#Alg) and the proportion of the generated REs its frequency represents (\%Alg). Finally, the accuracy (\%Acc) column compares the REs in the corpus with respect to the REs generated by the algorithm. The accuracy is the proportion of perfect matches between the algorithm output and the human REs from the corpus. The accuracy metric has been used in previous work for comparing the output of a REG algorithm with the REs found in corpora~\cite{sluis07:eval,viet:gene11} and is considered an strict comparison metric for this task. 

\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
RE & \#Cor & \%Cor & \#Alg & \%Alg & \%Acc \\
\hline
ball,green & 91 & 65 & 5887 &58,52 & 58,52 \\
ball,green,small & 23 & 16,43 & 2270 & 22,58 & 16,43 \\
ball,green,small,on-top(blue,cube,large) & 8 & 5,71 & 0 & 0 & 0 \\
ball,green,on-top(blue,cube) & 5 & 3,57 & 516 & 5,10 & 3,57 \\
ball,green,on-top(blue,cube,large) & 5 & 3,57 & 0 & 0 & 0 \\
ball,green,on-top(blue,cube),small & 2 & 1,43 & 1151 & 11,51 & 1,43 \\
ball,on-top(cube) & 1 & 0,71 & 20 & 0,20 & 0,20 \\
ball,green,small,on-top(blue,cube,large,left) & 1 & 0,71 & 0 & 0 & 0 \\
ball,on-top(cube,large),small	& 1 & 0,71 & 1 & 0,01 & 0,01 \\
ball,green,on-top & 1 & 0,71 &	26 & 0,26 & 0,26 \\
ball,on-top(cube), small & 1 & 0,71 & 18 & 0,18 & 0,18 \\
ball,green,on-top(cube) & 1 & 0,71 & 0 & 0 & 0 \\
ball,green,left	& 0 & 0 & 36 & 0,36 & 0 \\
ball,on-top(blue,cube) & 0 & 0 & 32 & 0,32 & 0 \\
ball,on-top & 0 & 0 & 22 & 0,22 & 0 \\
ball,green,small,on-top & 0 & 0 & 14 & 0,14 & 0 \\
ball,green,left,on-top(blue,cube),small & 0 &  0 & 7 & 0,07 & 0 \\
\hline
Total & 140 & 100 & 10000 & 100 & 80,6 \\
\hline
\end{tabular}
\caption{Referring expressions produced by the algorithm for Figure~\ref{GRE3D7-stimulus}\label{results-algo-fig3}}
\end{center}
\end{table}

In order to put our results in perspective we compare the accuracy obtained for several of the figures in the corpus using the probability of used inferred (column Learned \puse) and the probability of used directly extracted from corpora (column Extracted \puse) as explained in Section~\ref{sec:learning}. We show the accuracy results in Table~\ref{results-algo-all}. The random baseline (column Random) is calculated in by producing random probabilities of use and then running the algorithm 10000 using these random probabilities. Not only the intersection between the randomly generated REs and the corpus is lower but also many of  the REs generated in this way are not naturally sounding (e.g. ``small on the top of a blue cube that is below of something that is small''). 

\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Figure & Random & Learned \puse & Extracted \puse \\
\hline
Fig 1 & 5.38\% & 80.60\% & 84.27\% \\
Fig 3 & 3.29\% & 80.74\% & 83.19\% \\
Fig 6 & 12.76\%	& 84.00\% & 90.71\% \\
Fig 8 & 15.19\%	& 73.62\% & 85.14\% \\
Fig 10 & 7.94\%	& 60.85\% & 81.77\% \\
Fig 12 & 37.94\% & 84.15\% & 75.00\% \\
Fig 13 & 3.48\%	& 53.81\% & 93.57\% \\
Fig 21 & 7.92\%	& 82.00\% & 92.55\% \\
\hline
Average	& 11.74\% & 74.97\% & 85.77\% \\
\hline
\end{tabular}
\caption{Percentage on intersection between the REs generated using random probabilities, learned probabilities and probabilities directly extracted from the figure corpora\label{results-algo-all}}
\end{center}
\end{table}

