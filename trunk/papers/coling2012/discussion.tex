\section{Discussion}
\label{sec:discussion}

In this paper we extend Areces et al.~\shortcite{arec2:2008:Areces} algorithm to generate REs similar to those produced by humans. The extensions done to the algorithm are based on two observations, on the one hand, there is no ordering of properties that is able to generate all the REs produced by humans and humans frequently overspecify their REs~\cite{Engelhardt_Bailey_Ferreira_2006,Arts_Maes_Noordman_Jansen_2011}. We obtain an algorithm that is able to generate a large proportion of the overspecified REs found in corpora without generating bad referring expressions for our domain.

Viethen~\shortcite{viet:gene11} trains decision trees that are able to achieve a 65\% average accuracy on the GRE3D7 corpus, the same corpus that we use for our experiments. The decision trees are able to generate overspecified relational descriptions. Viethen proposal has the problem that the generated descriptions may not be referring expressions, that is, the decision trees do not consider a complete model of the scene that is being described and hence cannot make sure that the generated description are distinguishing.

Algorithms that generate overspecified and distinguishing referring expressions have been proposed~\cite{delucena-paraboni:2008:ENLG,ruud-emiel-mariet:2012:INLG2012}. All these algorithms have been not been evaluated on the GRE3D7, so our results are not directly comparable to them. They have been evaluated on the TUNA-AR corpus~\cite{gatt-balz-kow:2008:ENLG} where~\cite{delucena-paraboni:2008:ENLG} achieves a 33\% accuracy and the~\cite{ruud-emiel-mariet:2012:INLG2012} achives a 40\%. The TUNA-AR corpus includes only propositional REs, it would be interesting to evaluate how these algorithms perform in corpora of relation REs such as GRE3D7. 

Our algorithm is able to generate relational referring expressions which may include redundant information. It achieves a 75\% average accuracy on the GRE3D7 corpora. Our algorithm not only achieves a good correlation with the distribution of REs found in corpora but the mechanism that is uses in order to produce overspecification is inspired in psycholinguistic results in egocentrism~\cite{keysar:Curr98}. In Section~\ref{sec:algorithm} we describe that the generation of REs is performed in two steps. In the first iteration, the probability of including a property in the RE is dependent only on the \puse of that property. A property can be included in a RE even if it does not eliminate any distractor. Hence, the resulting RE may be overspecified. After all properties are given a chance of being included in this way, if the resulting RE is not distinguishing, then the algorithm enters a second stage in which it makes sure that the RE identifies the target uniquely. In this stage, overspecification is not allowed. We designed the algorithm in this way generalizing from the observation that, when producing language, considering the hearers point of view is not done from the outset but it is more of an afterthought~\cite{keysar:Curr98}. Keysar et al.~argue that adult speakers produce REs egocentrically like children do but then, if they have time, they adjust the REs so that the addressee is able to identify the target unequivocally. This first egocentric step is a heuristic process based in a model of saliency of the scene that contains the target. 

Our definition of \puse is intended to capture the different saliences of the properties for different scenes and targets. The notion of saliency that we use, represented by \puse, intends to capture the saliency of a property in a given scene. In our view, the \puse of a property changes according to the scene, it is not constant in a domain, as we discussed in Section~\ref{subsec:learning}. This is in contrast with previous work where the saliency of a property is constant in a domain. Keysar et al.~argues that the reason for this procedure of generate-and-adjust may have to do with the information processing limitations of the mind. They hypothesize that, if the heuristic that guides the egocentric first pass is well tunned, this egocentric pass succeeds in most cases but, sometimes, it leads to revisions. Interestingly, we observe that when we run the algorithm on learned values of \puse, the algorithm generates REs much faster than when we run it with random values. 

Our short term plans of future work include evaluating our algorithm on a more complex domain such as Open Domain Folksonimies~\cite{pacheco-duboue-dominguez:2012:NAACL-HLT}. Moreover, we also plan to explore interactive corpora such as the GIVE Corpus~\cite{GarGarKolStr10} where we have informally observed that, under time pressure, people first produce an underspecified RE that includes salient properties such as ``the red button'' and then, in a following utterance, they add ``to the left of the lamp'' identifying the target uniquely. 

