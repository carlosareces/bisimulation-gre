\section{Discussion} \label{sec:discussion}

In this article we extend~\shortcite{arec2:2008:Areces} algorithm to generate REs similar to those produced by humans. The modifications 
proposed are based on two observations. First, it has been argued that no fixed ordering of properties is able to generate all the REs produced by humans and, second, humans frequently overspecify their REs~\cite{Engelhardt_Bailey_Ferreira_2006,Arts_Maes_Noordman_Jansen_2011}. We tested 
the proposed algorithm on the GRE3D7 corpus and found that it is able to generate a large proportion of the overspecified REs found in the corpus without generating trivially redundant referring expressions.

\shortcite{viet:gene11} trains decision trees that are able to achieve a 65\% average accuracy on the GRE3D7 corpus. 
The approach based on decision trees is able to generate overspecified relational descriptions, but they might fail to be referring 
expressions. Indeed, as the decision trees does not verify the extension of the generated expression over a model of the scene, the 
generated descriptions might not uniquely identify the target, and they might even be innacurate [CHEQUEAR.]  As we have already discussed,
our algorithm ensures termination and it always finds a referring expression if one exists.  Moreover, it achieves an average of 75.03\% of accuracy over the scenes used in our tests. 

Different algorithm for the generation of overspecified and distinguishing referring expressions has been proposed in recent years 
(see, e.g.,~\cite{delucena-paraboni:2008:ENLG,ruud-emiel-mariet:2012:INLG2012}.  But, to our knowledge, they have not been evaluated on the 
GRE3D7 corpus and, hence, comparison is difficult. \shortcite{delucena-paraboni:2008:ENLG} and \shortcite{ruud-emiel-mariet:2012:INLG2012} algorithm
have been evaluated on the TUNA-AR corpus~\cite{gatt-balz-kow:2008:ENLG} where they have achieved a 33\% and 40\% accuracy respectively. 
As the TUNA-AR corpus includes only propositional REs, it would be interesting future work to evaluate how these algorithms perform in corpora with relational REs such as GRE3D7. 

As we discussed in Section~\ref{sec:overspecification}, 


Our algorithm not only achieves a good correlation with the distribution of REs found in corpora but the mechanism that is uses in order to produce overspecification is inspired in psycholinguistic results in egocentrism~\cite{keysar:Curr98}. As we described in Section~\ref{sec:overspecification} the generation of overspecified REs is performed in two steps. In the first iteration, the probability of including a property in the RE is dependent only on the \puse\ of that property. A property can be included in a RE even if it does not eliminate any distractor. Hence, the resulting RE may be overspecified. After all properties are given a chance of being included in this way, if the resulting RE is not distinguishing, then the algorithm enters a second stage in which it makes sure that the RE identifies the target uniquely. In this stage, overspecification is not allowed. We designed the algorithm in this way generalizing from the observation that, when producing language, considering the hearers point of view is not done from the outset but it is more of an afterthought~\cite{keysar:Curr98}. Keysar et al.~argue that adult speakers produce REs egocentrically like children do but then, if they have time, they adjust the REs so that the addressee is able to identify the target unequivocally. This first egocentric step is a heuristic process based in a model of saliency of the scene that contains the target. 

Our definition of \puse\ is intended to capture the different saliences of the properties for different scenes and targets. The notion of saliency that we use, represented by \puse, intends to capture the saliency of a property in a given scene. In our view, the \puse\ of a property changes according to the scene, it is not constant in a domain, as we discussed in Section~\ref{subsec:learning}. This is in contrast with previous work where the saliency of a property is constant in a domain. Keysar et al.~argues that the reason for this procedure of generate-and-adjust may have to do with the information processing limitations of the mind. They hypothesize that, if the heuristic that guides the egocentric first pass is well tunned, this egocentric pass succeeds in most cases but, sometimes, it leads to revisions. Interestingly, we observe that when we run the algorithm on learned values of \puse, the algorithm generates REs much faster than when we run it with random values. 


