\section{Learning to describe new objects from corpora}
\label{sec:learning}

In the previous section we presented an algorithm that assumes that each relation R 
used in a referring expression has a known probability of use R.\puse. 
Intuitively, the R.\puse is the probability of using relation R to describe the target. For example, in Figure~\ref{Tuna-scene} the probability of using blue to describe the target, which is surrounded by a red square, is higher than the probability of using the fact that the chair is facing left although both are properties of the target. The probability of using green is not null because a green object may be used in a relational description of the target (for example, the blue chair far from the green fan). In this section, 
we describe how to calculate these probabilities from corpora.  
The general set up is the following: we assume available a corpus of REs associated 
to different scenes that are prototypical of the domain in which the GRE algorithm has to operate.   
We show first how to calculate R.\puse\ values for those scenes for which a corpus of REs is available.  
We then show how to generalize these values to 
other scenes in the domain, using a machine learning algorithm. We exemplify the methodology using 
the TUNA-corpus that is introduced in the Section~\ref{sec:tuna}.

\subsection{The TUNA-corpus: A corpus of referring expressions}
\label{sec:tuna}
The TUNA Corpus~\cite{Gatt:2008:TCO:1708322.1708365} is a set of human-produced referring expressions (REs) for entities in visual domains of pictures of furniture and people as exemplified in Figures~\ref{Tuna-people-scene} and~\ref{Tuna-furniture-scene}. The corpus was
collected during an online elicitation experiment in which subjects typed descriptions of a target single referent or pair of referents. 
In each picture there were 5 or 6 other objects. 
%In the experiment, the participation was not controlled, but there was a main condition manipulated the +/-LOC: in +LOC condition, participants were told that they could refer to entities using any of their properties (including their location on the screen). In the -LOC condition, they were discouraged from doing so, though not prevented. 
The attributes for each entity include properties such as an object's colour or a person's characteristic such as having dark hear.
In this paper we will use the singular part as of the TUNA corpus. The corpus contains 780 singular referring expressions divided
into 80\% training data, 20\% test. 

ROMI: ADD HERE HOW MANY PEOPLE WERE USED IN THE DATA COLLECTION. 

\begin{figure}[ht]
%\begin{minipage}{0.60\linewidth}
\centering
\includegraphics[width=0.8\textwidth]{images/tuna-people.jpg}

%\vspace*{-.4cm}
\caption{TUNA-corpus people scene}
\label{Tuna-people-scene}
%\end{minipage}
%\vspace*{-.38cm}
%\begin{minipage}{0.50\linewidth}
\centering
%\vspace*{.2cm}
%\includegraphics[width=\textwidth]{images/tuna.jpg}
%\vspace*{-.25cm}
%\vspace*{-.4cm}

%\caption{TUNA-corpus furniture scene}
%\label{Tuna-furniture-scene}
%\end{minipage}
\end{figure}
\subsection{Calculating \puse\ when a corpus for the scene is available}

The TUNA corpus has two different domains furniture and people, so for each domain we calculate \puse\ of each word in the domain, and then calculate for each scene with a machine learning approach a function of the \puse\ for each word of that new scene that was not used for train. 

The annotation of the TUNA corpus have just properties for objects and have not relations between them annotated, there is a relation given by the positions but is it not annotated as relational properties between objects.

%Suppose we want to automatically generate REs for target $t$ in a given scene, and that we do have available a corpus $C$ for scenes similar of REs in the same domain (this is exactly the kind of information we find in the TUNA-corpus).  
%We use the models provided by TUNA as input of our algorithm.  
%We aisle the scene for which we want to compute the \puse\ and use all another REs of scenes. Then we estimate the value of \puse\ for each of the relations in the model as the percentage of REs 
%in which the relation appears. I.e., 
\begin{equation}\label{eq1}
R.\puse = \frac{\# \mbox{ of REs in $C$ in which R appears}}{\# \mbox{ of REs in $C$}}.
\end{equation}

\noindent
This estimation is overly simplified and, for example, it does not differentiate between the properties of a target and the 
properties of a landmark object used in a relational RE to complete the description of the target, but in this case people don't use too much descriptions of lansmarks. It is extremely easy 
to compute, and we will see in Section~\ref{sec:evaluation} that it already produces natural REs that match those found in the corpus. 

To clarify the computation of R.\puse\ and the model $\gM$ associated to each scene we list the required steps in detail, 
and discuss how we carried them out in the TUNA corpus:

ESTAS COSAS... YO LAS SACARIA...

\begin{enumerate}
\item Tokenize the referring expressions and call the set of tokens $T$. In particular, multi-word expressions like ``on top of'' 
should be matched to a single token like \emph{ontop}.

\item Remove hyperonyms from $T$. E.g., if both \emph{cube} and \emph{thing} appear in $T$, delete \emph{thing}.

\item If the set of tokens obtained in the previous steps contains synonyms normalize them to a representative in the synonym class, 
and call the resulting set $\REL$; it will be the signature of the model $\gM$ used by the algorithm. E.g., the tokens \emph{little} 
and \emph{small} are both represented by the token \emph{small}.

\item For each scene, define $\gM$ such that the interpretation $\interp{\cdot}$ ensures that all the REs in the corpus are REs in the model.
 E.g., the $\el$ formulas corresponding to the REs in Table~\ref{corpus-distribution} should all denote the target in the model $\gM$ 
depicted in 
Figure~\ref{Tuna-scene}.

\item For each R $\in \REL$ compute R.\puse\ using~(\ref{eq1}).\\[-1.9em]

\end{enumerate}

Steps 1-5 above are easy to carry out (actually, the tokenization and normalization steps were already done in the TUNA corpus). 
Starting from the scene in Figure~\ref{Tuna-scene}
the resulting signature and their associated \puse\ are listed in the first three columns of Table~\ref{probability-of-use}. 

Notice that the values R.\puse\ obtained in this way should be interpreted as the probability of using R to describe the target in model 
$\gM$, and we could argue that they are correlated to the \emph{saliency} of R in the model.  

For that reason, for example, the value of \emph{chair}.\puse\ is (AGREGAR ACA), while the value of \emph{fan}.\puse\ is (AGREGAR ACA)  
These probabilities will not be useful to describe different targets in different scenes. We will see how we can use them to obtain
 values for new targets and scenes using a machine learning approach in the next section. Not surprisingly, using these values for 
R.\puse\ the REs generated most often by the algorithm can be found in the corpus. More interestingly, as we discuss in 
Section~\ref{sec:evaluation} the algorithm generates REs with a distribution that matches the one found in the corpus and, 
as Table~\ref{results-algo-fig3} shows, even the generated REs not found in the corpus are natural.    


\subsection{Calculating \puse\ for each word of scenes training with another scenes} \label{subsec:learning}

The learning was done with the machine learning toolkit WEKA~\cite{Hall:WEK09}, training on all minus one (the one for that we are learning) for all the scenes of the TUNA-corpus. We use linear regression to learn the function of \puse\ for each word in the signature. 
For a given scene, we replace the variables of the obtained function by the values of the features in the scene that we want to obtain the \puse\ for describe. 
We use simple features to obtain the function, all the features can be extracted automatically from the relational model and are listed 
in Table~\ref{features}.  

\begin{small}
\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|p{10cm}|}
\hline
target & whether the target element has the property \\
\#rel-prop & number of properties and relations that the target has\\
\#rel & number of the relations that the target has \\
landmark & whether a landmark of the target has the property, an object is a landmark if there is a direct relation in the model 
between them \\
discrimination & 1 over the number of objects in the model that have the property \\
\hline
\end{tabular}
\caption{Features used for learning the \puse for each token in the signature of the scenes of the TUNA-corpus} 
\label{features}
\end{center}
\end{table}
\end{small}
Our feature set is intentionally simplistic in order for it to be domain independent. As a result there are some complex relations 
between characteristics of the scenes that it is not able to capture. The most important characteristic of our domain is that we are not able 
to learn, and has an impact in our performance, we show that we couldn't learn the dependency of dimension-x and dimension-y, it mean, when a person adds dimension-x is highly probably that he includes dimension-y in his referring expression. ACA PUEDO PONER UNA TABLA DE PEOPLE PARA QUE QUEDE MAS LENO, O HACERLA MAS CHIQUITA A LA LETRA, O PONER SOLO LAS PROPIEDADES QUE HAY EN EL DOMINIO.
\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|c|}
\hline
Token for Model Fig. \ref{Tuna-scene} & \puse learned\\
\hline
colour-blue & 0.8864 \\
colour-green & 0.0036 \\
colour-gray & 0.0 \\
colour-other & 0.0063 \\
colour-red & 0.0044 \\
orientation-back & 0.0238 \\
orientation-front & 0.0503 \\
orientation-left & 0.2486 \\
orientation-right & 0.0 \\
other-other & 0.0353 \\
size-large & 0.211039 \\
size-other & 0.0031 \\
size-small & 0.04620 \\
type-chair & 0.9371 \\
type-desk & 0.0 \\
type-fan & 0.0 \\
type-other & 0.047 \\
type-sofa & -0.0053 \\
x-dimension-1 & -0.0284 \\
x-dimension-2 & 0.004 \\
x-dimension-3 & 0.0039 \\
x-dimension-4 & 0.0 \\
x-dimension-5 & 0.2712 \\
y-dimension-1 & -0.0213 \\
y-dimension-2 & 0.0 \\
y-dimension-3 & 0.2941 \\
\hline
\end{tabular}
\caption{Probabilities of use of the tokens from the corpora for scene 101t5} 
\label{probability-of-use}
\end{center}
\end{table}

\textit{Using} linear regression we are able to learn interesting characteristics of the domain. To start with, it learns known facts such that the 
saliency of a color depends strongly on whether the target object is of that color, and it does not depend on its discrimination power 
in the model. Moreover, (VER SI DEJAMOS ESTO) it learns that the on-top relation is used more frequently than the horizontal relations (left-of and right-of) 
which confirms a previous finding reported in~\cite{viet:gene11}. Finally, it learned a surprising fact of the TUNA corpus
 (not found by previous work), that is that size is used more frequently in an overspecified manner(VERIFICAR SI ESTO SE CUMPLE EN TUNA) when the target and landmark share the
 size. (REESCRIBIR ESTA PARTE, LA NOCION DE LANDMARK NO ES TAN USADA EN TUNA)Size was used in overspecified REs when in 49\% of the descriptions for scenes where target and landmark shared the size, and 25\% 
of the time when target and landmark did not share the size. This can be explained by the observation that if landmark and target share a
 property, this property is more salient. 

We did a comparison of our results with the results in the challenge 2008, but how we had just one RE to compare we did a human evaluation that we will explain in the next chapter.

Our algorithm is capable of give more than one RE but like comparison will be not fair we just use the more frequent RE given by the algorithm in 100 runnings.

