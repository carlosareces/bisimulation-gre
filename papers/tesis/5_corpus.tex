\chapter{Recolecci\'on y an\'alisis del corpus ZOOM}
\label{sec:corpus}

\textcolor{blue}{tengo el problema de que empezamos a hacer el corpus para 3 idiomas, y lo terminamos haciendo para 2, no se si explicarlo... o sacar de la imagen de la pagina la parte ingles}

El corpus ZOOM fue un experimento realizado en un trabajo conjunto con la Universidad de S\H ao Paulo Brasil, a fin de crear un corpus de expresiones referenciales (ER) bilingual y con 2 niveles de zoom de un dominio mucho m\'as natural a las aplicaciones del mundo real que los existentes hasta el momento.\\

En esta tesis se quiere mostrar que el algoritmo de bisimulaci\'on con probabilidades de uso de las palabras a partir de corpus podr\'a generar ER en un dominio que es directamente aplicable a aplicaciones de inteligencia artificial.\\
\textcolor{blue}{reescribir cuando este terminado}
En este Cap\'itulo se describe el dominio y las caracter\'isticas del corpus, luego en Secci\'on~\ref{corpus-voluntarios} se dar\'an estad\'isticas de las personas que completaron el experimento, en la Secci\'on~\ref{corpus-metodo} se explicar\'a en detalle que se le pidi\'o a las personas involucradas en el experimento, en la Secci\'on~\ref{corpus-materiales} se mostrar\'an los materiales usados, y en la Secci\'on~\ref{corpus-anotacion} se hablar\'a de como se anot\'o el corpus, que expresiones se descartaron. Para finalizar el Cap\'itulo discutiremos la fiabilidad de obtener expresiones con este m\'etodo en la Secci\'on~\ref{corpus-discusion} y luego haremos una evaluaci\'on en Secci\'on~\ref{corpus-evaluacion} de los datos obtenidos.\\

\section{M\'etodo de recolecci\'on del corpus}

%\subsection{Caracter\'isticas del corpus}
%\label{corpus-caracteristicas}

La recolecci\'on se llevo a cabo mediante una p\'agina web en la que registramos 20 ER dichas por cada persona. Cada persona di\'o 22 ER de mapas distintos, los primeros 2 mapas eran solamente para que la persona se acostumbre a usar el sistema, 11 de los cuales ten\'{i}an target singular, es decir s\'olo 1 target y los otros 11 target plural, es decir ten\'{i}an 2 targets.

%Se dise\~n\'o un experimento basado en la web para recopilar descripciones en lenguaje natural de ubicaciones de mapas en espa\~nol y en portugu\'es. 
El conjunto de datos obtenidos constituye un corpus de expresiones referenciales para investigaci\'on en REG y campos relacionados.

Las situaciones de referencia en cuesti\'on hacen uso de mapas con dos grados de detalle (representados por los niveles de zoom X y 2X), se pidieron descripciones en singular y plural, tuvimos en cuenta un gran n\'umero de personas (unos 100 hablantes de cada idioma).


\textcolor{blue}{esta parte la tenia en caracteristicas del corpus, no se si dejarla suelta aca...}
El origen de los mapas usados es openstreetmaps, de ah\'i obtuvimos fragmentos de las ciudades Lisboa y Madrid.\\

Openstreetmaps.org es una p\'agina que tiene informaci\'on de mapas de todas partes del mundo, es una organizaci\'on en la que muchas
 personas de distintas partes del mundo colaboran para mantener la informaci\'on actualizada. Esta informaci\'on es de libre uso, es decir uno puede usar los mapas y solo hay que nombrar de donde fueron sacados.\\

Se consideraron 2 idiomas: espa\~nol y portugu\'es. Se obtuvieron ER para targets singulares como se ve por ejemplo en (Figuras~\ref{rest-singular} y~\ref{rest-singular2x}) y plurales en (Figuras~\ref{rest-plural} y~\ref{rest-plural2x}), en los plurales se usaron referencias del mismo tipo (2 restaurantes, 2 iglesias, etc.) y adem\'as se tuvo en cuenta mapas con 2 niveles de zoom, X como se muestra en las (Figuras~\ref{rest-singular} y~\ref{rest-plural}) y 2X como se muestra en las (Figuras~\ref{rest-singular2x} y~\ref{rest-plural2x}).\\

\begin{figure}

\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/rest-singular.png}\\[0pt]
\vspace*{.1cm}
\caption{Target singular con zoom X}
\label{rest-singular}
\end{minipage}
\hspace*{0cm}
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/rest-plural.png}\\[0pt]
\vspace*{.1cm}
\caption{Target plural con zoom X}
\label{rest-plural}
\end{minipage}
\end{figure}

\begin{figure}
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/rest-singular2x.png}\\[0pt]
\caption{Target singular con zoom 2X}
\label{rest-singular2x}
\end{minipage}
\vspace*{.1cm}
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/rest-plural2x.png}\\[0pt]
\caption{Target plural con zoom 2X}
\label{rest-plural2x}
\end{minipage}
\end{figure}



%.....................................
\subsection{Sobre las personas que completaron el experimento}
%.....................................
\label{corpus-voluntarios}

Las ER fueron hechas por personas voluntarias a las cuales se les envi\'o una invitaci\'on por correo electr\'onico o redes sociales. La parte en Portugu\'es del corpus tuvo 93 participantes, siendo 66 (71,0 \%) hombres y 27 (29,0 \%) mujeres. El corpus espa\~nol tuvo 85 participantes, siendo 59 hombres (69,4 \%) y 26 mujeres (30,6 \%).
\textcolor{blue}{aca puedo agregar informacion estadistica, como ser edades desde hasta, mayormente de tal pais}
%.....................................
\subsection{Procedimiento}
%.....................................
\label{corpus-metodo}

Las personas voluntarias recibieron un link a la interfaz del experimento, como se muestra en la Figura~\ref{fig-pagPrincipal} esa p\'agina conten\'{i}a las instrucciones en 3 idiomas: Ingl\'es, Portugu\'es y Espa\~{n}ol, y un link para completar el experimento en el idioma seleccionado, luego de seleccionar el idioma se mostraba la siguiente p\'agina, como en la Figura~\ref{fig-nacionalidadGenero} en la que se ped\'{i}a completar, nacionalidad, edad, g\'enero, y se mostraban los t\'erminos y condiciones ``Acepto de que los datos ingresados en este cuestionario sean usados an\'onimamente para investigaci\'on'', si la persona completaba los datos y aceptaba se comenzaba el emperimento mostrando una imagen similar a la Figura~\ref{fig-interface} que tambi\'en conten\'{i}a las instrucciones, con solo pasar el mouse por la palabra instrucciones se mostraban las mismas.
Las instrucciones dec\'ian lo siguiente:\\
``Su tarea es describir el o los lugares (restaurantes, iglesias, etc) indicados por una o dos flechas rojas en una serie de mapas, completando la frase; ``Es interesante visitar...'' como si estuviera dando indicaciones a un amigo. Intente ser preciso en sus respuestas no escribiendo solamente ``el restaurante'' o ``las iglesias'', pero no se preocupe en dar una descripci\'on demasiado detallada: s\'olo tiene que seguir su primera intuici\'on''

Lo que se pretend\'ia con esas instrucciones era por un lado que la persona de una expresi\'on referencial del objeto o los objetos sen\~alados que sea un sintagma nominal y por otro lado que la expresi\'on fuera natural como le dar\'ia a un amigo.

La p\'agina tambi\'en ten\'{i}a una barra indicadora de progreso. Los mapas fueron mostrados en forma aleatoria y al final del experimento se mostraba un mensaje de agradecimiento.

Cada mapa mostraba un lugar determinado (por ejemplo, un restaurante, pub, teatro, etc.) se\~nalado por una flecha (o dos, en el caso de los plurales). Para cada escena, se pidi\'o a los voluntarios que imaginen que estaban dando consejos a un amigo y que deb\'{i}an completar la frase ``Ser\'{i}a interesante visitar ...'' con una descripci\'on del lugar se\~nalado por la flecha (o las flechas). Despu\'es de pulsar el bot\'on 'Siguiente', se seleccionaba otro est\'{i}mulo al azar y as\'{i} hasta el final del experimento.


Cada vez que se presionaba el bot\'on ``Siguiente'' la p\'agina evaluaba las siguientes cosas:  si hab\'ia respuesta, en el caso de no haber, aparec\'ia un cartel que dec\'ia ``Falta respuesta''.
Si la persona ponia ``teatro'' o ``el teatro'' el sistema le respondia ``No se entiende a que teatro se est\'a refiriendo. Por favor sea m\'as espec\'ifico'', estas excepciones concretas fueron manejadas a travez del c\'odigo  de la p\'agina, pero no hubo ningun an\'alisis de la expresi\'on ingresada, solo la comparaci\'on con cadenas est\'aticas.
%Las dos primeras im\'agenes eran rellenos exclusivamente destinados para que los voluntarios se familiarizaran con el entorno del experimento, y no se registraron las respuestas correspondientes. 

Las descripciones mal formadas fueron descartados siguiendo los criterios que explicamos aqu\'{i}. Como se ve en la Fig.~\ref{fig-interface} las personas ten\'{i}an que completar la frase ``Ser\'{i}a interesante visitar ...'' con un sintagma nominal que describe la ubicaci\'on se\~nalada por la flecha (o las flechas en los casos plurales). Sin embargo, algunas de las ER no eran frases nominales, sino frases completas. Por ejemplo, las respuestas tales como ``Vamos a ir a pizza express, es realmente barato'' fueron descartados debido a que la persona ten\'{i}a en mente un objetivo comunicativo que no era la identificaci\'on del objetivo. Por otra parte, algunas personas completaron el experimento con frases como ``No me gusta la comida r\'apida ''. Del mismo modo, hemos eliminado todas las expresiones que describen un objeto que no sea el target previsto, las que utilizaban propiedades que no eran ciertas para el target, todas las situaciones en que la persona ten\'{i}a que describir dos objetos pero describen s\'olo una, y todas las descripciones que contienen s\'olo el b\'asico tipo de atributo (como en ``la iglesia'').



%.....................................
\subsection{Materiales}
%.....................................
\label{corpus-materiales}

El experimento hizo uso de la interfaz especialmente dise\~nada la cual se muestra en la Figura~\ref{fig-interface}. En esa interface se observa el texto ``Es interesante visitar...'', abajo hay un espacio para que la persona ingrese la expresi\'on referencial, al lado esta el bot\'on ``Siguiente'' que  al presionarlo se guardaban los datos ingresados para el est\'imulo actual y se continuaba con el siguiente est\'imulo, de el lado derecho hab\'ia una barra de estado que mostraba el progreso hasta el momento, cada vez que se presionaba el bot\'on ``Siguiente'' la barra se actualizaba llen\'andose con color verde y mostrando el porcentaje que luego de 22 mapas llegaba al 100\%, debajo del indicador de progreso hab\'ia un link que dec\'ia ``Instrucciones'', el cual al pasar el cursor del mouse desplegaba las instrucciones mostradas al inicio.
Del lado derecho del mapa se encontraba la leyenda, que mostraba para un conjunto de \'iconos del mapa que objetos significaban, se mostraron los siguientes (biblioteca, teatro, bar, caf\'e, fast-food, restaurante, farmacia, pub, iglesia, cajero, banco), esto se realiz\'o a fin de evitar confusi\'on a las personas que daban las expresiones referenciales, ya que se not\'o por ejemplo que los \'iconos de cajero y banco podr\'ian llegar a ser identificados como la misma palabra para distintas personas. 

El mapa conten\'ia \'iconos de lugares o cosas, calles, nombres de lugares, por ejemplo podemos ver en el mapa de la Figura~\ref{fig-interface} la Calle Mayor, ahi se pueden ver cajeros, restaurantes, fast-foods, cafe, telefono, correo, farmacia, semaforos. Notar que algunos \'iconos tienen nombre, por ejemplo el fast-food McDonald's. Algunos edificios o plazas tambi\'en tienen nombre como la Real Casa de Correos y la Plaza de Pontejos. En el mapa tambi\'en podemos ver algunos objetos sin nombre.

Se puede ver en las Figura~\ref{rest-singular} el nombre de la iglesia Parroquia de Santa Cruz, pero en la Figura~\ref{rest-singular2x} no se muestra el nombre, se ve solamente el \'icono. Pero para el Ministerio de Asuntos Interiores se ve el nombre en ambos niveles de zoom. Notar que no siempre en el nivel 2X de zoom se ver\'a todo lo que se ve y mas que en el nivel X, por ejemplo tenemos el restaurante Medina Mayrit, cuyo nombre se ve en Figura~\ref{rest-singular} pero no se ve en Figura~\ref{rest-singular2x}. 

%Suponemos que esto tiene que ver con la superposici\'on de nombres, 
Los mapas eran fragmentos del centro de la ciudad de Madrid (para la parte espa\~nola del corpus) y de Lisboa (para la parte portuguesa).

Para cada ciudad, se utilizaron 10 mapas con distintas ubicaciones. Cada ubicaci\'on se muestra con un nivel X y 2X de zoom, haciendo 20 im\'agenes en total. En ambos niveles de zoom el objetivo se\~{n}alado se mantuvo.%, pero en la versi\'on m\'as detallada, por supuesto, se ve un mayor n\'umero de distractores y detalles adicionales en general. %Algunos nombres de las calles y de landmarks podi\'{i}an no aparecer en diferentes niveles de zoom.

La mitad de las im\'agenes mostraron una sola flecha que apunta a un objeto en el mapa (es decir, requer\'{i}an una sola descripci\'on como por ejemplo en la Figura~\ref{rest-singular} ``el restaurante en la Calle de Atocha, cerca del Ministerio de Asuntos Exteriores''), mientras que en la otra mitad mostr\'o dos flechas que apuntaban a dos lugares diferentes (y por lo tanto requer\'ian una referencia a un conjunto, como en Figura~\ref{rest-plural} ``los dos restaurantes de la Calle de Atocha'' o ``el restaurante cerca del Ministerio de Asuntos Exteriores y el Medina Mayrit'').


\subsection{Anotaci\'on del corpus}
\label{corpus-anotacion}
\textcolor{blue}{Aca poner imagen, que muestre los id}
- Cada objeto en el mapa fue marcado con un identificador \'unico (Ej. rest3, pub2 etc.)

- Los valores de los atributos se anotaron en Ingl\'es (por ejemplo, restaurant, pub, street, etc.), pero los nombres propios (para calles, etc.) se mantuvieron en su forma original, tal cual se v\'e en el mapa (Espa\~{n}ol, Portugu\'es).

- Cada objeto tiene 26 posibles atributos.

- En el caso de las descripciones plurales, el conjunto de atributos se repite para cada objeto, por lo que el anotador pod\'{i}a utilizar hasta 52 atributos.

- 10 atributos denotan el objeto principal (target):
  tipo (por ejemplo, restaurante)
  nombre (por ejemplo, McDonalds)
  en (calle-id, avenida-id, etc.)
  izquierda (obj-id)
  derecha (obj-id)
  en esquina / entre (Street1, Street2)
  cerca (obj-id)
  en-frente-de (obj-id)
  detr\'as (obj-id)
  Otros (cualquier otra cosa mencionada en la descripci\'on)

- Los otros 16 atributos denotan los objetos adicionales que se mencionan en la descripci\'on, a los cuales llamamos d1..d4. Por ejemplo, en ``El restaurante que esta junto a la iglesia, en la calle Chapel'') tenemos: target = restaurante, d1 = church (Iglesia) y d2 = Chapel-Street (Calle Chapel).

- Para cada taget se consider\'o un m\'aximo 4 puntos de referencia (landmarks) adicionales d1..d4 nombrados en la ER, de cada landmark se anotaron 4 atributos:
  landmark Identificaci\'on (Id)
  landmark Tipo de referenca (un sustantivo, restaurant, pub...)
  landmark nombre
  landmark otro (alguna otra cosa dicha sobre el objeto)

- Cada atributo tiene una lista estricta de los valores permitidos. Por ejemplo, para el objetivo en nuestro Mapa 1 los valores posibles para el tipo de atributo son s\'olo dos: ``restaurante'' u ``otro''. El valor ``otro'' le da al anotador una oportunidad de dar una respuesta cuando la descripci\'on muestra algo muy inusual.


- Para los atributos espaciales (izquierda, cerca, etc.) permitimos que los valores que denota la mayor\'{i}a de los objetos en el mapa, a excepci\'on de aquellos que son claramente ilegales. Por ejemplo, en el caso de la ``izquierda'' posibles valores incluyen la mayor\'{i}a de los objetos en la pantalla, a excepci\'on de los de la derecha del objeto de destino.
%MOVER A OTRO LADO ESTA SECCION
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	       
%\section{Background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	       
%\label{sec-background}

%In this section we briefly discuss a number of REG corpora publicly available for research purposes, and how these resources compare to our current work.

%TUNA \cite{tuna-corpus} was the first prominent REG corpus to be made publicly available for research purposes. The corpus was developed in a series of general-purpose controlled experiments, containing 2280 descriptions produced by 60 speakers in two domains (1200 descriptions of furniture items and 1080 descriptions of people's photographs). TUNA does not contain relational descriptions, and it is possibly the only resource of this kind to include situations of reference to sets. The TUNA corpus has been extensively used in a series of shared tasks \cite{reg2009}.

%TUNA \cite{tuna-corpus} fue el primer prominente corpus REG para estar disponible al p\'ublico para fines de investigaci\'on. El corpus fue desarrollado en una serie de experimentos controlados de prop\'osito general, que contiene 2.280 descripciones producidas por 60 altavoces en dos dominios (1.200 descripciones de art\'{i}culos de los muebles y 1080 en las descripciones de las fotograf\'{i}as de las personas). AT\'UN no contiene descripciones relacionales, y es posiblemente el \'unico recurso de este tipo para incluir situaciones de referencia a los conjuntos. El corpus AT\'UN se ha utilizado ampliamente en una serie de tareas compartidas \cite{reg2009}.



%GRE3D3 and its extension GRE3D7 \cite{gre3d3,gre3d7} were developed in a series of web-based experiments primarily focussed on the study of relational descriptions. GRE3D3 contains 630 descriptions produced by 63 speakers, and GRE3D7 contains 4480 descriptions produced by 287 speakers, making it the largest of its kind to date. The GRE3D domain consists of simple visual scenes containing only two kinds of objects (boxes and spheres) with limited variation in colour and size. In each scene, there is only one possible spatial relation between target and the nearest landmark. Both corpora contain atomic and relational descriptions.


%GRE3D3 and its extension GRE3D7 \cite{gre3d3,gre3d7}
%se desarrollaron en una serie de experimentos basados en la Web se centraron principalmente en el estudio de las descripciones relacionales. GRE3D3 contiene 630 descripciones producidas por 63 altavoces, y GRE3D7 contiene 4.480 descripciones producidas por 287 oradores, por lo que es el mayor de su tipo hasta la fecha. El dominio GRE3D consta de escenas visuales simples que contienen s\'olo dos tipos de objetos (cajas y esferas) con variaci\'on limitada en color y tama\~no. En cada escena, s\'olo hay una posible relaci\'on espacial entre el objetivo y el hito m\'as cercano. Ambos cuerpos contienen descripciones at\'omicas y relacionales.

%Stars \cite{stars-mutual-disamb} and its extension Stars2 were collected for the study of referential overspecification (particularly in the case of relational descriptions). Stars was developed in a pilot web-based experiment, containing 704 descriptions produced by 64 speakers.  The more comprehensive Stars2 data set was produced in dialogue situations involving subject pairs, and it contains 884 descriptions produced by 56 speakers. Both domains make use of simple visual scenes containing up to four object types (e.g., stars, boxes, cones and spheres) with limited variation in colour and size. Differently from other REG corpora, however, Stars/2 includes a considerable number of complex situations of reference involving up to three objects, as in `the box near the sphere, next to the cone'. 


%Stars \cite{stars-mutual-disamb} y su extensi\'on Stars2 se recogieron para el estudio de sobrevaloraci\'on referencial (particularmente en el caso de las descripciones relacionales). Estrellas se desarroll\'o en un experimento basado en la web piloto, que contiene 704 descripciones producidas por 64 altavoces. El conjunto de datos Stars2 m\'as completa se produjo en situaciones de di\'alogo que implican pares sujetos, y contiene 884 descripciones producidas por 56 altavoces. Ambos dominios hacen uso de escenas visuales simples que contienen hasta cuatro tipos de objetos (por ejemplo, estrellas, cajas, conos y esferas) con variaci\'on limitada en color y tama\~no. A diferencia de otros REG corpus, sin embargo, Estrellas / 2 incluye un n\'umero considerable de situaciones complejas de referencia que incluyan hasta tres objetos, como en `la caja cerca de la esfera, al lado del cono".


%Despite their usefulness and general contribution to the research in REG, the above domains are still at a certain distance from the kinds of visual scene that might be required for a practical, real-world application. The need for additional complexity and/or realism, and our own interest in the surface realisation task for the Spanish and Portuguese languages, has led us to build a new computational resource of this kind. This work is described in the next sections. Further discussion on the differences between the Zoom corpus and existing resources is presented in Sec. 


%A pesar de su utilidad y contribuci\'on general a la investigaci\'on en REG, los dominios anteriores se encuentran todav\'{i}a en una cierta distancia de los tipos de escena visual que podr\'{i}an ser necesarias para una aplicaci\'on en el mundo real pr\'actico. La necesidad de complejidad y / o realismo adicional, y nuestro propio inter\'es en la tarea realizaci\'on de superficie para los idiomas espa\~nol y portugu\'es, nos ha llevado a construir un nuevo recurso computacional de este tipo. Este trabajo se describe en las siguientes secciones. Continuaci\'on del debate sobre las diferencias entre el corpus Zoom y los recursos existentes se presenta en la Sec.\ref{sec-annotation}. 



%.....................................
%\subsection{El corpus y la anotaci\'on}
%.....................................
%\label{sec-annotation}

%The experiment website was kept online until 100 complete trials were obtained for Portuguese and 80 complete trials were obtained for Spanish. Upon manual verification, 602 ill-formed Portuguese descriptions and 366 Spanish descriptions were discarded. Thus, the Portuguese portion of the corpus consists of 1358 descriptions while the Spanish portion contains 1234 referring expressions. In Section~\ref{sec-problems} we describe the criteria by which descriptions were considered ill-formed. We also discuss the challenges faced when collecting natural language descriptions in a web-based experiment for a domain that is significantly closer to real-world applications than existing REG corpora. 

%In the Portuguese portion of the data, 78.6\% of the descriptions include relational properties. In addition to that, 36.4\% were minimally distinguishing, 44.3\% were overspecified, and  19.3\% were underspecified. In the Spanish portion of the data, 70\% of the descriptions include relational properties. Moreover, 35\% were minimally distinguishing, 40\% were overspecified, and 25\% were underspecified. Underspecified descriptions as well as relational descriptions are not common in existing REG corpora - certainly not in this proportion.  

%Table \ref{tab-comparison} presents a comparison between the collected data and existing REG corpora. The domain information represents the number of possible atomic attributes and the number of relations in each description. The information on TUNA and Zoom descriptions is based on the singular portion of each corpus only. This represents the average description size (in number of annotated properties) and properties usage, which is taken to be  the proportion of properties that appear in the description over the total number of possible attributes and landmarks. From a REG perspective, larger description sizes and lower usage scores are likely to represent more complex situations of reference.

%IP: I realise that what I meant to show here was not the number of possible relations in each corpus (which in some cases is very large - below, above, near, nextto, etc.) but simply the maximum number of relations that may appear in each description. This is the same as reporting the number of landmark objects in addition to the main target. I fixed this now by changing the text and column title to ``landmarks', and I also updated the Zoom rows from 7 relations to 4 landmarks. The data regarding the other corpora was already reflecting the number of landmarks rather than the number of relations, so no further changes were required.

La p\'agina web del experimento se mantuvo en l\'{i}nea hasta que se obtuvieron 100 ensayos completos para el portugu\'es y 85 ensayos completos para el espa\~nol. Tras la verificaci\'on manual, se descartaron 602 descripciones portugueses mal formadas y 366 descripciones espa\~nolas. As\'{i}, la parte portuguesa del corpus consta de 1.358 descripciones mientras que la parte espa\~nola contiene 1.234 ER. En la Secci\'on~\ref{sec-problemas} se describen los criterios por los que se consideraron las descripciones mal formada.\textcolor{blue}{reescribir esta parte} Tambi\'en se discuten el desaf\'{i}o que supone obtener las ER en lenguaje natural en un experimento basado en la web para un dominio que es significativamente m\'as cerca de aplicaciones del mundo real que los corpus REG existentes.

En la parte portuguesa de los datos, el 78,6\% de las descripciones incluyen propiedades relacionales. Adem\'as de eso, el 36,4\% eran m\'{i}nimamente distintivo, un 44,3\% eran sobreespecificadas, y el 19,3\% eran underespecificada\textcolor{blue}{spanishhhhh word}. En la parte espa\~nola de los datos, 70\% de las descripciones incluyeron propiedades relacionales. Por otra parte, el 35\% eran minimales, el 40\% eran sobreespecificadas, y el 25\% era underespecificadas\textcolor{blue}{spanishhhh word}. 
Esta proporci\'on tan grande de descripciones underespecificadas\textcolor{blue}{spanishhhh word} as\'{i} como descripciones relacionales no son comunes en corpora REG existente.

Tabla \ref{tab-comparison} presenta una comparaci\'on entre el corpus recolectado y corpus de RE existentes. La informaci\'on de dominio representa el n\'umero de posibles atributos at\'omicos y el n\'umero de relaciones en cada descripci\'on. La informaci\'on sobre el TUNA-corpus y las descripciones de ZOOM se basan s\'olo en la parte singular de cada corpus. Esto representa el tama\~no medio de la descripci\'on (en n\'umero de propiedades anotadas) y la utilizaci\'on de las propiedades, la cual se toma como la proporci\'on de las propiedades que aparecen en la descripci\'on sobre el n\'umero total de posibles atributos y puntos de referencia.\textcolor{blue}{reescribir- Desde una perspectiva de REG, las ER de mayor tama\~no y las puntuaciones m\'as bajas de uso es probable que representen las situaciones m\'as complejas de referencia.}


\begin{table}[ht]
\begin{center}
\footnotesize{
\caption{Comparison with existing REG corpora}
\label{tab-comparison}
\begin{tabular} {  l c c c c}
\hline
%\multicolumn{1}{c}{}
%&\multicolumn{1}{c}{Domain}
%&\multicolumn{3}{c}{Descriptions}\\
Corpus											& Atributos			& Landmarks			& Tama\~{n}o promedio	& Uso \\
\hline
TUNA-Furniture							& 4								& 0							&	3.1				& 0.8   \\
TUNA-People									& 10							& 0							& 3.1				& 0.3   \\
GRE3D3											&	9								& 1							& 3.4				& 0.3   \\
GRE3D7											&	6								& 1							& 3.0				& 0.4   \\
Stars												&	8								& 2							& 4.4				& 0.4   \\
Stars2											& 9								& 2							& 3.3				& 0.3   \\
Zoom-Pt											& 19							& 4							& 6.7				& 0.3   \\
Zoom-Sp											& 19							& 4							& 7.2				& 0.3   \\
\hline
\end{tabular}
}
\end{center}
\end{table}


%Annotation was performed as follows. Each referring expression was modelled as conveying a description of the main target object and, optionally, up to four descriptions of related landmarks. The annotation scheme consisted of three target attributes, four landmark attributes for each of the four possible landmark objects, and seven relational properties. This makes 26 possible attributes for each referring expression. In the case of plural descriptions (i.e., those involving two target objects), this attribute set is doubled.

%Every object was annotated with the atomic attributes {\em type}, {\em name} and {\em others} and, in the case of landmark objects, also with their {\em id}. In addition to that, seven relational properties were considered: {\em in/on/at}\footnote{The three prepositions were aggregated as a single attribute because they have approximately the same meaning in the languages under consideration}, {\em next-to}, {\em right-of}, {\em left-of}, {\em in-front-of}, {\em behind-of}, and the multivalue relation {\em between} intended to represent `corner' relations. 

%Possible values for the {\em type} and {\em name} attributes are predefined by each referential context. The {\em others} attribute may be assigned any string value, and it is intended to represent any non-standard piece of information conveyed by the referring expression. For the spatial relations, possible values are the object identifiers available from each scene.

%The collected descriptions were fully annotated by two independent annotators. After completion, a third annotator assumed the role of judge and provided the final annotation. Since the annotation scheme was fairly straightforward (i.e., largely because all non-standard responses were simply assigned to the {\em others} attribute), agreement between judges as measured by Kappa \cite{kappa} was 84\% at the attribute level. 

%Both referential contexts and referring expressions were represented in XML format using a simplified version of the XML format adopted in the TUNA corpus \cite{tuna-corpus}. Descriptions were grouped into TRIAL nodes containing general information about each subject (i.e., id, age and gender) followed by his/her list of responses. Each response identifies the context within which it was produced, and the referring expression proper. 

%As in \cite{tuna-corpus}, the contents of referring expressions are represented by ATTRIBUTE-SET nodes containing a list of attribute names and their values. The following example illustrates a fragment of this representation for an elicited description. Objects in each input context were represented in a similar fashion.

La anotaci\'on se realiz\'o como sigue. Cada ER da una descripci\'on del objeto target y, opcionalmente, hasta cuatro descripciones de landmarks relacionados. El esquema de anotaci\'on consisti\'o en tres atributos para el target, cuatro atributos para cada uno de los cuatro posibles landmarks, y siete propiedades relacionales. Esto hace 26 posibles atributos para cada expresi\'on referencial. En el caso de las descripciones plurales (es decir, los que implican dos objetos se\~{n}alados), este conjunto de atributos se duplica.

Cada objeto fue anotado con los atributos at\'omicos {\em tipo}, {\em nombre} y {\em otros} y, en el caso de objetos landmarks, tambi\'en con su {\em Identificaci\'on}. Adem\'as de eso, se consideraron siete propiedades relacionales: {\em en / sobre / a} \footnote{Las tres preposiciones fueron agregadas como un \'unico atributo, ya que tienen aproximadamente el mismo significado en los idiomas considerados}, {\ em pr\'oximo -para}, {\em derecho-de}, {\em izquierda de}, {\em en-frente-de}, {\em detr\'as de}, y la relaci\'on {\em entre} que pretende representar relaciones `esquina'.


Los valores posibles para los atributos {\em tipo} y {\em nombre} est\'an predefinidos por cada contexto referencial. En el atributo {\em otros} puede asignar cualquier valor de cadena, y se pretende representar cualquier pieza no est\'andar de la informaci\'on transmitida por la ER. Para las relaciones espaciales, los valores posibles son los identificadores de los objetos disponibles en cada escena.

Las descripciones recolectadas fueron totalmente anotadas por dos anotadores independientes. Luego, un tercer anotador asumi\'o el papel de juez y di\'o la anotaci\'on final. Dado que el esquema de anotaci\'on fue bastante sencillo (es decir, en gran parte porque las respuestas no estandares simplemente se asignan al atributo {\em otros}), se midi\'o el acuerdo entre los jueces, con Kappa \cite{kappa}, \'este fue del 84\% en el nivel de atributo.

Ambos, el contexto y las ER estuvieron representados en formato XML utilizando una versi\'on simplificada del formato XML adoptado en el corpus TUNA \cite{tuna-corpus}. Las descripciones se agruparon en nodos TRIAL que contienen informaci\'on general sobre cada persona (es decir, identificaci\'on, edad y sexo), seguido de su lista de respuestas. Cada respuesta identifica el contexto en que se produjo, y la expresi\'on en referencial que la persona dijo.

Al igual que en \cite{tuna-corpus}, el contenido de las expresiones referenciales est\'an representadas por nodos que son conjunto de atributos que contienen una lista de nombres de atributos y sus valores. El siguiente ejemplo ilustra un fragmento de esta representaci\'on para una ER. Los objetos en cada contexto de entrada estaban representados de una manera similar.


\tiny{
\begin{verbatim}
<TRIAL ID="2" SPEAKER="166" AGE="18" GENDER="m">

  <CONTEXT ID="3" SEQ="1300">
     <ATTRIBUTE-SET TARGET="lib3" LANDMARK="street3" 
                    STRING="the library at Cowgate">
        <ATTRIBUTE NAME="type" VALUE="library" />
        <ATTRIBUTE NAME="at" VALUE="street3" />
        <ATTRIBUTE NAME="landmark-name" VALUE="cowgate" />
     </ATTRIBUTE-SET>
  </CONTEXT>

  ...
	
</TRIAL>	
\end{verbatim}
}
\normalsize


%In each description, attribute names for the first landmark object are preceded by the `landmark-' label as in the above example. Subsequent landmarks were labelled as `second-landmark' and so on. This was motivated by the need to provide unique attribute names for the benefit of REG algorithms.

%The set of images, text descriptions and their XML representations constitutes the Zoom corpus of referring expressions, to be made publicly available for research purposes. As a first step in this direction, the following sections present the results of a machine learning approach to REG based on the Portuguese portion of the data.

En cada descripci\'on, el nombre para el primer landmark va precedido por la etiqueta `landmark-' como en el ejemplo anterior. Los landmarks siguientes fueron etiquetados como ``second-landmark'' y as\'{i} sucesivamente. Esto fue motivado por la necesidad de proporcionar nombres de atributos \'unicos para el beneficio de los algoritmos REG.

El conjunto de im\'agenes, textos descriptivos y sus representaciones XML constituye el corpus ZOOM de ER, el cual ser\'a p\'ublico para fines de investigaci\'on. Como primer paso en esta direcci\'on, las siguientes secciones se presentan los resultados de un enfoque de aprendizaje autom\'atico para REG, los cuales se basan en la parte portuguesa del corpus.


%.....................................
\subsection{Discusi\'on}
%.....................................
\label{corpus-discusion}

%Previous corpora of referring expressions have been collected in specially designed and highly controlled domains. Collecting corpora from a domain that is directly relevant to real-world applications poses a significant challenge not only to the collection and annotation process itself but also to existing REG algorithms. This is made evident by the large proportion of referring expressions that needed to be manually discarded from our dataset because the did not constitute a referring expression of the intended target. This proportion is 30\% for the Portuguese portion of the dataset and 23\% for the Spanish portion. 

%The ill-formed descriptions were discarded following the criteria that we explain here. As seen in Fig.~\ref{fig-interface} subjects had to complete the sentence ``It would be interesting to visit ...'' with a noun phrase describing the location signalled by the arrow. However, some of the referring expressions were not noun phrases but full sentences. For example, responses such as ``Let's go to pizza express, it's really cheap'' were discarded because the subject had in mind a communicative goal other than identifying the target. Moreover, a few subjects completed the experiment with sentences like ``I don't like fast food'' which were clearly out of domain. Similarly, we removed all expressions that described an object other than the intended target, which used properties that were not true of the target, all situations in which the subject had to describe two objects but described only one, and all descriptions containing only the basic attribute type (as in ``the church'').

%After this cleaning process the Portuguese portion of the corpus still contains a 19.3\% of underspecified referring expressions and the Spanish portion a 25\%. An example of underspecified referring expression is shown in Fig.~\ref{fig-interface}; the referring expression ``the pub at Cowgate'' is underspecified because there are two pubs at Cowgate street on the map. The percentage of underspecified referring expressions in the Zoom corpus is much higher than that reported in the corpora described in the the previous section, which is not higher than 5\%. Previous psycholinguistic studies ~\cite{Clark1986} have found that over 20\% of the referring expressions in naturally occurring discourse are underspecified with respect to the established context. In sufficiently complex domains, ~\cite{Clark1986} found that speakers will often produce an initial underspecified referring expression, and then repair it if necessary instead of producing a uniquely identifying description from the start. As discussed in 
%the previous section, the Zoom domain is likely contain more complex situations of reference if compared to existing resources of this kind, that is,  Zoom description are on average longer, and have a lower average attribute usage. This, in our opinion, may explain the high proportion of referential underspecification in our data.   

%The proportion of overspecified referring expressions of the Zoom corpus is similar to that found in previous work. However, the proportion of relational descriptions is presently higher. Relational descriptions constitute a challenge for REG algorithms due to the computational complexity associated to their generation~\cite{survey}. A further challenge posed by the Zoom corpus is the fact that it contains two descriptions for every target based on different - but related - models corresponding to the same map location seen with different zoom levels. For instance a map with higher zoom level (2X) is illustrated in Fig.~\ref{fig-with-zoom}, and the same map with lower zoom level is shown in Fig.~\ref{fig-interface}. 


\textcolor{blue}{Corpora anterior de expresiones referenciales se han recogido en los dominios de dise\~no especial y muy controladas. SACO ESTO}

La recopilaci\'on de corpus de un dominio que es directamente relevante para aplicaciones del mundo real plantea un reto importante no s\'olo para el proceso de recolecci\'on y anotaci\'on en s\'{i}, sino tambi\'en a los algoritmos REG existentes. Esto se hace evidente por la gran proporci\'on de las expresiones referenciales que deben ser desechadas desde nuestra base de datos porque no constituyen una ER para el objeto target. Esta proporci\'on es del 30\% de la parte portuguesa del copus y el 23 \% de la parte espa\~nola del corpus.


Despu\'es de este proceso de limpieza de la parte portuguesa del corpus contiene todav\'{i}a un 19,3 \% de expresiones que se refieren underspecified y la parte espa\~nola de un 25\%. Un ejemplo de ER underspecified se muestra en la figura ~\ref{fig-interface}.; la ER ``el pub en Cowgate'' se underspecified porque hay dos bares en la calle Cowgate en el mapa. El porcentaje de ER underspecified en el corpus Zoom es mucho mayor que lo reportado en la corpora descrito en el la secci\'on anterior, que no es mayor que 5\%. Estudios psicoling\"u\'{i}sticos anterior~\cite{Clark1986} han encontrado que m\'as del 20\% de las ER en el discurso de origen natural se underspecified con respecto al contexto establecido. En los dominios suficientemente complejos,~\cite{Clark1986} se encontr\'o que los hablantes a menudo producen una ER underspecified inicial, y luego reparar si es necesario en lugar de producir una descripci\'on identifica de forma un\'{i}voca desde el principio. Como se discuti\'o en el apartado anterior, el dominio de zoom es probable que contenga situaciones m\'as complejas de referencia si se compara con los recursos existentes de este tipo, es decir, la descripci\'on de zoom son en promedio m\'as tiempo, y tienen un uso atributo medio m\'as bajo. Esto, en nuestra opini\'on, puede explicar la alta proporci\'on de underspecification referencial en nuestros datos.

La proporci\'on de expresiones que se refieren falte espacio del corpus Zoom es similar a la encontrada en el trabajo anterior. Sin embargo, la proporci\'on de las descripciones relacionales es actualmente superior. Descripciones relacionales constituyen un desaf\'{i}o para los algoritmos REG debido a la complejidad computacional asociada a su generaci\'on~\cite{survey}. Otro desaf\'{i}o que plantea el corpus Zoom es el hecho de que contiene dos descripciones para cada objetivo bas\'andose en diferente - pero relacionados - modelos correspondientes a la misma ubicaci\'on mapa visto con diferentes niveles de zoom. Por ejemplo, un mapa con mayor nivel de zoom (2X) se ilustra en la Fig.~\ref{fig-con-zoom}, y el mismo mapa con un menor nivel de zoom se muestra en la Fig.~\ref{fig-interface}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=8.5cm]{figures/with-zoom.jpg}\\[0pt]
\caption{Mapa con nivel de zoom m\'as detallado}
\label{fig-with-zoom}
\end{center}
\end{figure}

%The underlying models for these two maps are different but not unrelated. The map with 2X zoom contains fewer objects but may include more properties due to the added level of detail. The  referring expression for the target in the 1X map may or may not be the same as in the 2X map. For instance, the referring expression  ``the pub at Cowgate'' is underspecified on the 1X map, but it is minimally distinguishing on the 2X map. Changes of this kind are common in interactive applications.  The context of reference during an interaction changes in structure, in the number of objects and referable properties. The challenge for REG algorithms would be to produced an appropriate description for the modified context without starting from scratch. REG algorithms based on local (rather than global) context partitioning (e.g., ~\cite{areces08}) seem to have an advantage in this respect, and in this sense we hope that the Zoom data will help the research in the field to move forward.

 Los modelos subyacentes de estos dos mapas son diferentes, pero no sin relaci\'on. El mapa con zoom 2X contiene menos objetos, pero pueden ser m\'as propiedades debido al mayor nivel de detalle. La ER para el target en el mapa 1X puede o no puede ser la misma que en el mapa 2X. Por ejemplo, la expresi\'on se refiere ``el pub en Cowgate'' se underespecificada en el mapa 1X, pero es minimal en el mapa 2X. Cambios de este tipo son comunes en aplicaciones interactivas. El contexto de referencia durante una interacci\'on cambios en la estructura, en el n\'umero de objetos y propiedades a las cuales referirse. El desaf\'{i}o para los algoritmos REG ser\'{i}a producir una descripci\'on apropiada para el contexto modificado sin empezar desde cero. REG algoritmos basados en local (no mundiales) partici\'on contexto (por ejemplo,~\cite{areces08}) parecen tener una ventaja en este sentido, y en este sentido esperamos que los datos de ZOOM ayudar\'an a que la investigaci\'on progrese.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluaci\'on}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{corpus-evaluacion}
\textcolor{blue}{decidir si esta parte la dejo o no, mas me gustaria poner evaluacion para la parte en espanol}

In this section we illustrate the use of the Zoom corpus as training and test data for a machine learning approach to REG adapted from \cite{thiago-svm}. The  goal of this evaluation is to provide reference results for future comparison with purpose-built REG algorithms, and not to present a complete REG solution for this or other domains.

%................................
\subsection{Modelo Computacional }
%................................

The present model consists of 12 binary classifiers representing whether individual referential attributes should be selected for inclusion in an output description. The classifiers correspond to atomic attributes of the target and first landmark object ({\em type}, {\em name} and {\em others}), and relations. Referential attributes of other landmark objects were not modelled due to data sparsity and also to reduce computational costs. For similar reasons, the multivalue {\em between} relation is also presently disregarded, and `corner' relations involving two landmarks (e.g., two streets) will be modelled as two separate classification tasks.

Two learning features were considered by each classifier: {\em landmarkCount}, which represents the number of landmark objects near the main target, and {\em DistractorCount}, which represents the number of objects of the same type as the target within the relevant context in the map.

From the outcome of the 12 binary classifiers, a description is built by considering atomic target attributes in the first place. For every positive prediction, the corresponding atomic attribute is selected for inclusion in the output description. Next, relations are considered. If no relation is predicted, the algorithm terminates by returning an atomic description  of the main target object. If the description includes a relation, the related landmark object is selected  as well, and the algorithm is called recursively to describe the next object.

Since every attribute that corresponds to a positive prediction is always selected, the algorithm does not regard uniqueness as a stop condition. As a result, the output description may convey a certain amount of overspecification.


El presente modelo se compone de 12 clasificadores binarios que representan si los atributos referenciales individuales deben ser seleccionados para su inclusi\'on en una descripci\'on de salida. Los clasificadores se corresponden con atributos at\'omicos del destino y objeto primer hito ({\ em tipo}, {\ nombre em} y {\ em otros}), y las relaciones. Atributos referenciales de otros objetos emblem\'aticos no se modelaron debido a la escasez de datos y tambi\'en para reducir los costos computacionales. Por razones similares, el valor m\'ultiple {\ em entre} relaci\'on es tambi\'en actualmente caso omiso, y `relaciones esquina 'que implican a dos puntos de referencia (por ejemplo, dos calles) se modelan como dos tareas de clasificaci\'on independientes.

Dos caracter\'{i}sticas de aprendizaje fueron considerados por cada clasificador: {\ em landmarkCount}, que representa el n\'umero de objetos se\~nal cerca de la diana principal, y {\ em DistractorCount}, que representa el n\'umero de objetos del mismo tipo que el objetivo dentro de la pertinente contexto en el mapa.

A partir de los resultados de los 12 clasificadores binarios, una descripci\'on se construye teniendo en cuenta los atributos de destino at\'omicas en el primer lugar. Por cada predicci\'on positiva, se selecciona el atributo at\'omica correspondiente para su inclusi\'on en la descripci\'on de salida. A continuaci\'on, se consideran las relaciones. Si se prev\'e ninguna relaci\'on, el algoritmo termina devolviendo una descripci\'on at\'omica del objeto de destino principal. Si la descripci\'on incluye una relaci\'on, se selecciona el objeto de punto de inter\'es relacionadas, as\'{i}, y el algoritmo se llama de forma recursiva para describir el siguiente objeto.

Puesto que cada atributo que corresponde a una predicci\'on positiva est\'a siempre seleccionado, el algoritmo no considera singularidad como una condici\'on de parada. Como resultado, la descripci\'on de salida puede transmitir una cierta cantidad de sobrevaloraci\'on.

%......................
\subsection{Procedure}
%......................

We used a subset of singular descriptions from the Portuguese portion of the corpus. This comprises  821 descriptions produced in 9 scenes. Evaluation was carried out by comparing the corpus description with the system output to measure overall accuracy (i.e., the number of exact matches between the two descriptions), Dice \cite{dice} and MASI \cite{masi} coefficients (i.e., the degree of overlap between the algorithm output and the corresponding corpus description).

Following \cite{thiago-svm}, we built a REG model using support vector machines with radial basis function kernel. The classifiers were trained and tested using 6-fold cross validation. Optimal parameters were selected using grid search as follows: for each step in the main cross-fold validation, one fold is reserved for testing, and the remaining $k-1$ folders are subject  to a second cross-validation procedure in which different parameter combinations are attempted. The $C$ parameter is assigned the values 1, 10, 100 and 1000, and $\gamma$ is assigned 1, 0.1, 0.001 and 0.0001. The best-performing parameter set is selected to build a classifier trained from the $k-1$ folders, and tested on the test data. This procedure is repeated for every iteration of the main cross-validation procedure.

Se utiliz\'o un subconjunto de descripciones singulares de la parte portuguesa del corpus. Esto comprende 821 descripciones producidos en 9 escenas. La evaluaci\'on se llev\'o a cabo mediante la comparaci\'on de la descripci\'on corpus ante la salida del sistema para medir la precisi\'on global (es decir, el n\'umero de coincidencias exactas entre las dos descripciones), dados \ cite {} dados y MASI \ cite {} masi coeficientes (es decir, el grado de solapamiento entre la salida del algoritmo y la descripci\'on corpus correspondiente).

Siguiendo \ cite {thiago-svm}, construimos un modelo REG uso de m\'aquinas de vectores soporte con radial kernel de funci\'on de base. Los clasificadores se capacit\'o y sometieron a pruebas de 6 veces la validaci\'on cruzada. Los par\'ametros \'optimos fueron seleccionados mediante b\'usqueda en la red de la siguiente manera: para cada paso en el principal validaci\'on cruzada veces, un solo reba\~no est\'a reservado para las pruebas, y los restantes $ k-1 $ carpetas est\'an sujetas a un segundo procedimiento de validaci\'on cruzada en la que diferentes par\'ametros combinaciones se intentan. El par\'ametro $ $ C se le asignan los valores de 1, 10, 100 y 1000, y $ \ gamma $ es asignado en 1, 0,1, 0,001 y 0,0001. El conjunto de par\'ametros de mejor desempe\~no es seleccionada para construir un clasificador entrenado desde las carpetas $ k-1 $, y probado en los datos de prueba. Este procedimiento se repite para cada iteraci\'on del procedimiento principal de validaci\'on cruzada.
%......................
\subsection{REG Results}
%......................

Table \ref{tab-reg-results} summarizes the results obtained by the REG algorithm built from SVM classifiers, those obtained by a baseline system representing a relational extension of the Dale \& Reiter Incremental Algorithm, and by a Random selection strategy.  
Tabla \ ref {tab-REG-resultados} resume los resultados obtenidos por el algoritmo REG construido a partir de SVM clasificadores, los obtenidos mediante un sistema de l\'{i}nea de base que representa una extensi\'on relacional de la \& Reiter Incremental Algoritmo Dale, y por una estrategia de selecci\'on aleatoria.

% these results are for SVM.All.VAR-   compared to the AEI- baseline
\begin{table}[ht]
\begin{center}
\caption{REG results}
\label{tab-reg-results}
\begin{tabular} {  l c c c }
\hline
{Algorithm}							& {Acc.} 	& { Dice}		& MASI \\ \hline 
SVM											& 0.15		&	0.51			& 0.28 \\
Incremental							& 0.04		&	0.53			& 0.21 \\
Random selection       	& 0.03    & 0.45      & 0.15 \\
\hline
\end{tabular}
\end{center}
\end{table}

In what follows we compare accuracy scores obtained by every algorithm pair using the chi-square test, and we compare {\em Dice} scores using {\em Wilcoxon's} signed-rank test.

In terms of overall accuracy\footnote{And also in terms of MASI scores, although this is presently not further discussed.}, the SVM approach outperforms both alternatives. The difference from the second best-performing algorithm (i.e., the Incremental approach) is highly significant ($\chi^{2}=$ 79.87, df=1, p$<$0.0001). Only in terms of Dice scores an opposite effect is observed (T=137570.5, p$=$ 0.01413). 

We also assessed the performance of the individual classifiers. Table \ref{tab-svm-results} shows these results as measured by precision (P), recall (R), F1-measure (F1) and area under the ROC curve (AUC). 


En lo que sigue se comparan precisi\'on resultados obtenidos por cada par algoritmo mediante la prueba de chi cuadrado, y comparamos {\ em dados} puntajes utilizando la prueba de rangos con signo {\ em de Wilcoxon}.

En t\'erminos de precisi\'on global \ footnote {Y tambi\'en en cuanto a las puntuaciones MASI, aunque actualmente no se discute m\'as.}, El m\'etodo SVM supera a ambas alternativas. La diferencia con el segundo algoritmo de mejor rendimiento (es decir, el enfoque incremental) es altamente significativo ($ \ chi ^ {2} = $ 79,87, df = 1, p $ <$ 0,0001). S\'olo en cuanto a las puntuaciones de los dados se observa el efecto contrario (T = 137570.5, p $ = $ 0,01413).

Tambi\'en se evalu\'o el desempe\~no de los clasificadores individuales. Tabla \ ref {tab-SVM-resultados} muestra estos resultados, medida por la precisi\'on (P), memoria (R), F1-medida (F1) y el \'area bajo la curva ROC (AUC).

%these are the rsults for the training over the set of ALL speakers 
\begin{table}[ht]
\begin{center}
\footnotesize{
\caption{Classifier results}
\begin{tabular}{l c c c c }
\hline
{{Classifier}}	& {P} & {R} & {$F_{1}$} & {AUC} \\
\hline
{{tg\_type}} 			& 0.95 & 1.00 & 0.98 & 0.25 \\
{{tg\_name}}			& 0.09 & 0.05 & 0.07 & 0.41 \\
{{tg\_other}}			& 0.00 & 0.00 & 0.00 & 0.05 \\                               
{{lm\_type}}			& 0.93 & 1.00 & 0.96 & 0.44 \\                               
{{lm\_name}}			& 0.97 & 1.00 & 0.98 & 0.35 \\                               
{{lm\_other}}			& 0.00 & 0.00 & 0.00 & 0.43 \\                               
{{next-to}}				& 0.50 & 0.24 & 0.32 & 0.63 \\                               
{{right-of}}			& 0.00 & 0.00 & 0.00 & 0.28 \\                               
{{left-of}}				& 0.00 & 0.00 & 0.00 & 0.27 \\                               
{{in-front-of}}		& 0.00 & 0.00 & 0.00 & 0.42 \\                               
{{behind-of}}			& 0.00 & 0.00 & 0.00 & 0.17 \\                               
{{in/on/at}} 			& 0.60 & 0.60 & 0.60 & 0.61 \\                               
\hline                   
\end{tabular}
\label{tab-svm-results}
}
\end{center}
\end{table}
\normalsize

From these results we notice that highly frequent attributes (e.g., type and name) were classified  with high accuracy, whereas others (e.g., multivalue attributes and relations) were not. 

A partir de estos resultados nos damos cuenta de que los atributos muy frecuentes (por ejemplo, el tipo y nombre) se clasificaron con gran precisi\'on, mientras que otros (por ejemplo, los atributos de varios valores y relaciones) no lo eran.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	       >2
\section{Final remarks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	       >2
\label{sec-final}

This paper has introduced the Zoom corpus of natural language descriptions of map locations. The corpus takes into account a domain that is significantly closer to real-world applications, and addresses more complex situations of reference than existing resources of this kind. In particular, the Zoom domain makes use of  contexts with different levels of detail, and contains instances of singular and plural reference produced by a relatively large number of speakers in both Spanish and Portuguese languages.

Preliminary results of a SVM-based approach to REG - which were solely presented for the future assessment of REG algorithms based on Zoom data - hint at the actual complexity of the REG task in this domain in a number of ways. 

First, we notice that a similar SVM-based approach in \cite{thiago-svm} on GRE3D3 and GRE3D7 data has obtained considerably higher mean accuracy (0.46 and 0.64 respectively) and higher mean Dice scores (0.78 and 0.89 respectively). This is of course explained by the increased complexity of the Zoom domain (e.g., in number of objects and possible atomic and relational properties etc.). However, we notice also that the Zoom annotation scheme - although taking into account an already large number of possible attributes - is still overly simple. In particular, the attribute {\em others} has been overused and, as a result, the chances of reproducing the corpus descriptions is narrow. Clearly, more work needs to be done to expand the annotation scheme and split {\em others} into a larger number of attributes.

Second, Zoom descriptions are prone to convey relations between a single target and multiple landmark objects, as in `the restaurant on 5th street, near the library'. Although common in language use, the use of multiple relational properties in this way has been little investigated, perhaps because it may lead to ambiguous descriptions, as in `the restaurant near the church on Jackson street and the bar on 5th street' for a restaurant located near the junction of these two streets.

As future work, we intend to make the corpus publicly available for research on multiple aspects of content selection and surface realisation in REG. In particular, we envisage the use of the Zoom corpus as training and test data for machine learning models of REG, and also to address further issues that have not been addressed in the present paper. These include  the generation of referring expressions from knowledge bases with different degrees of detail, and the issues of between-speakers and within-speakers variation in REG.

En este trabajo se ha presentado el corpus de zoom de las descripciones en lenguaje natural de localizaciones en el mapa. El corpus tiene en cuenta un dominio que es significativamente m\'as cerca de aplicaciones del mundo real, y aborda las situaciones m\'as complejas de referencia de los recursos existentes de este tipo. En particular, el dominio de zoom hace uso de contextos con diferentes niveles de detalle, y contiene ejemplos de referencia singular y plural producido por un n\'umero relativamente grande de hablantes en ambos idiomas espa\~nol y portugu\'es.

Los resultados preliminares de un enfoque basado en SVM a REG---que fueron \'unicamente presenta para la futura evaluaci\'on de algoritmos REG basados en datos Zoom---alusi\'on a la complejidad real de la tarea REG en este dominio en un n\'umero de maneras.

En primer lugar, nos damos cuenta de que un enfoque basado en SVM similar en \~cite{thiago-svm} en GRE3D3 y GRE3D7 datos ha obtenido considerablemente mayor precisi\'on media (0,46 y 0,64 respectivamente) y una mayor media de las puntuaciones de los dados (0,78 y 0,89 respectivamente). Esto es, por supuesto, se explica por el aumento de la complejidad del dominio Zoom (por ejemplo, en el n\'umero de posibles objetos y propiedades at\'omicas y relacionales etc.). Sin embargo, nos damos cuenta tambi\'en que el esquema de anotaci\'on Zoom \- aunque teniendo en cuenta la ya gran n\'umero de posibles atributos - sigue siendo demasiado simple. En particular, el atributo {\ em dem\'as} ha sido usado en exceso y, como consecuencia, las posibilidades de reproducci\'on de las descripciones corpus es estrecho. Es evidente que a\'un queda trabajo por hacer para ampliar el esquema de anotaci\'on y dividida {\ em otros} en un mayor n\'umero de atributos.

En segundo lugar, las descripciones de zoom son propensos a transmitir las relaciones entre un solo objetivo y varios objetos emblem\'aticos, como en `el restaurante en la calle 5, cerca de la biblioteca '. Aunque es com\'un en el uso del lenguaje, el uso de m\'ultiples propiedades relacionales de esta manera ha sido poco investigado, tal vez debido a que puede dar lugar a descripciones ambiguas, como en `el restaurante cerca de la iglesia en la calle Jackson y el bar en la calle 5 'para un restaurante situado cerca del cruce de estas dos calles.

Como trabajo futuro, tenemos la intenci\'on de hacer que el corpus a disposici\'on del p\'ublico para la investigaci\'on sobre m\'ultiples aspectos de la selecci\'on de contenidos y la realizaci\'on de superficie en REG. En particular, prevemos el uso del corpus zoom como datos de entrenamiento y prueba de modelos de aprendizaje autom\'atico de REG, y tambi\'en para abordar otras cuestiones que no se han abordado en el presente trabajo. Estos incluyen la generaci\'on de expresiones referenciales de bases de conocimiento con diferentes grados de detalle, y las cuestiones de la variaci\'on entre los altavoces y dentro-hablantes en REG.

