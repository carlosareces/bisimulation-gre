\section{Learning to describe new objects from corpora}
\label{sec:learning}

In the previous section we presented an algorithm that assumes that each property used in the referring expressions has a known probability of use. In this section, we describe how to calculate these probabilities from corpora when there is one available, and how to estimate them if there is no such corpora. This section starts by describing the corpora that we use for calculating the probabilities of use. Then we present the method used in the calculation. And finally we propose a methodology for estimating the probability of use of properties when describing objects for which no corpora is available. 

\subsection{A corpus of referring expressions}

The corpus used in this paper is known as the GRE3D7 corpus and consists of 4480 referring expressions. This corpus is the largest corpus of distinguishing descriptions developed to date. These referring expressions describe objects in 32 stimuli 3D scenes. Each scene contains a small number of simple objects (cubes and balls), and the individual descriptions were elicited in the absence of a preceding discourse. The stimulus scenes are designed in a way that encourage the use of relations between objects, but do not require them. For a detailed description of the collection produre see~\cite[Chapter 5]{viet:gene11}. A sample stimulus used in the corpus collection is shown in Figure~\ref{GRE3D7-stimulus} (the target object is marked with an arrow). Table~\ref{corpus-distribution} shows the REs that appear in the corpus for Figure~\ref{GRE3D7-stimulus} together with their number of occurrences in the corpus and the percentage that they represent.  

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Referring expressions & Occurrences & Percentage \\
\hline
green ball & 91 & 65.00\% \\
small green ball & 23 & 16.43\% \\
small green ball on top of large blue cube & 8 & 5.71\% \\
green ball on top of blue cube & 5 & 3.57\% \\
green ball on top of large blue cube & 5 & 3.57\% \\
small green ball on top of blue cube & 2 & 1.43\% \\
ball on top of cube & 1 & 0.71\% \\
small green ball on top of large blue cube to the left & 1 & 0.71\% \\
small ball on top large cube & 1 & 0.71\% \\
top green ball & 1 & 0.71\% \\
small ball on top of small cube & 1 & 0.71\% \\
green ball on top of cube & 1 & 0.71\% \\
\hline
\end{tabular}
\caption{Referring expressions produced by the subjects for Figure~\ref{GRE3D7-stimulus}\label{corpus-distribution}}
\end{center}
\end{table}

The REs in the corpus were produced by 294 participants, each participant produced 16 referring expressions corresponding to 16 different scenes. In this way, 140 descriptions for each of the 32 scenes were obtained resulting in a corpus of 4480 REs in total. This corpus is particularly well suited for the goal of this paper, namely, studying non-deterministic choices while selecting the content of the REs, since it contains input from many different speakers. 


\subsection{Calculating the probability of use}

In order to calculate the probability of use required by the algoritm from corpora we propose the following methodologies depending of the kind of corpora that exists.

If there exists corpora of referring expressions for the target object that wants to be described we follow the next steps. First we tokenize the referring expressions into attributes. This process is similar to a typical string tokenization in which each word is matched to an attribute---e.g., ``green''---except that relational attributes are multiword expressions matched to a single token----e.g., ``on top of''. Second, the vocabulary of the stimulus is obtained by identifying the disctinct tokens that appear in the REs. Third, if the vocabulary contains synonyms---e.g., box and cube---we choose one of them and normalize the REs to include only one of synonym chosen. Finally, we define the probability of use of a token as the percentage of REs in which the token appear in a given stimuli. This vocabulary is the signature of the model that is used as input of the algoritm as described in Section~\ref{sec:algorithm}.  

The three steps of pre-processing the corpora were already done in the GRE3D7 corpora semi-automatically so we did not need to do them ourselves. The REs shown in Table~\ref{corpus-distribution} had already gone throught the preprocessing steps. The resulting vocabulary and their associated probabilities of use obtained from the corpus are shown in Table~\ref{probability-of-use}. As you can observe from the table, the probability of use does not specify the absolute preference of an attribute over another for any scene in the domain. As an example notice that ``cube'' has a probability lower than ``small'' althogh, in general, taxonomic properties such as ``cube'' are preferred over size properties such as ``small''. The probabilities of use calculated in this manner give a higher probability to the properties of the target. The advantage of the simplicity of our approach to calculating probabilities of use in this way is that their calculation can be automatized if necessary with ease. 

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Token & Probability of use \\
\hline
ball & 1.0 \\
green & 0.978 \\
small & 0.257 \\
on-top & 0.178 \\ 
cube & 0.178 \\
blue & 0.15 \\
large & 0.107 \\
left & 0.007 \\
top & 0.186 \\
\hline
\end{tabular}
\caption{Probabilities of use of the tokens from the corpora in Table~\ref{GRE3D7-stimulus}\label{probability-of-use}}
\end{center}
\end{table}

What is the purpose of generating REs if one already has REs of the target in that scene? The purpose is have an algorithm that generates REs that could have been in the corpus according to the distribution of properties observed. As we will show in Section~\ref{sec:evaluation}, the algorithm will be able to generate REs like ``small ball on top of blue cube'' to describe the target in Figure~\ref{corpus-distribution}, which could have naturally appeared in the corpora. In the following section we describe how to calculate the probability of use if no such specific corpora is available. 

\subsection{Learning to describe new objects}

If there is no corpus on the target scene we need a corpus that uses the same kind of attributes and we apply the following methodology. In order to generalize from the corpora we need it to be annotated with the kinds of attributes that their tokens represent. In the domain of the corpus GRE3D7 we use the kind of attributes defined in~\cite{viet:gene11}. 

\textbf{Romi: Add a paragraph here explaining how we do the learning}

\textbf{Romi: Add a table here that describes the features used in the learning}

\textbf{Romi: Insert here a table with the kind of attributes, tokens of this kind, and learned probability of use}
