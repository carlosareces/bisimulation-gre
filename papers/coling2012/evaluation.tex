\section{Evaluation}
\label{sec:evaluation}

In this section we present a quantitative evaluation of the algorithm proposed in this paper. In the GRE area there was a common assumption that there is a gold standard ordering for a given domain~\cite{Dale1995}. However, this assumption has been dropped after empirical studies such as those presented in~\cite{arec2:2008:Areces,viet:gene11}. It has been observed that not only there is no single ordering of properties that covers all human-produced descriptions in a given domain but, in fact, it is not even the case that each speaker consistently uses just one ordering. In this section we show that the non-deterministic algoritm presented in the previous section is able to generate a distribution of REs similar to that observed in corpora, even when no corpus specific for a target object is available. 

Using the probabilities of used learned as described in Section~\ref{sec:learning} and running our algorithm 10000 times, we obtain 21 referring expressions. We generate 8 of the 12 different kinds of REs observed in the 140 ocurrences of the corpora. We also generate other 13 REs for the target, not present in the corpora, but natural sounding as can be observed in Table~\ref{results-algo-fig3} that only represent a 1,64\% of the utterances generated by the algorithm. Hence, 98,36\% of the utterances generated by the algorithm appear in the corpora.  

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
RE & \#Cor & \%Cor & \#Alg & \%Alg & \%Int \\
\hline
ball, green & 91 & 65 & 5852 &58,52 & 58,52 \\
ball, green, small & 23 & 16,43 & 2258 & 22,58 & 16,43 \\
ball, green, small, on-top(blue,cube,large) & 8 & 5,71 & 0 & 0 & 0 \\
ball, green, on-top(blue,cube) & 5 & 3,57 & 510 & 5,10 & 3,57 \\
ball, green, on-top(blue,cube,large) & 5 & 3,57 & 0 & 0 & 0 \\
ball, green, on-top(blue,cube), small & 2 & 1,43 & 1151 & 11,51 & 1,43 \\
ball, on-top(cube) & 1 & 0,71 & 20 & 0,20 & 0,20 \\
ball, green, small, on-top(blue,cube,large,left) & 1 & 0,71 & 0 & 0 & 0 \\
ball, on-top(cube,large), small	& 1 & 0,71 & 1 & 0,01 & 0,01 \\
ball, green, top & 1 & 0,71 &	26 & 0,26 & 0,26 \\
ball, on-top(cube), small & 1 & 0,71 & 18 & 0,18 & 0,18 \\
ball, green, on-top(cube) & 1 & 0,71 & 0 & 0 & 0 \\
ball,green,left	& 0 & 0 & 36 & 0,36 & 0 \\
ball,front,green & 0 & 0 & 35 & 0,35 & 0 \\
ball,on-top(blue,cube) & 0 & 0 & 32 & 0,32 & 0 \\
ball,top & 0 & 0 & 22 & 0,22 & 0 \\
ball,green,small,top & 0 & 0 & 14 & 0,14 & 0 \\
ball,front,top & 0 & 0 & 7 & 0,07 & 0 \\
ball,green,left,on-top(blue,cube),small & 0 &  0 & 7 & 0,07 & 0 \\
ball,front,green,on-top(blue,cube) & 0 & 0 & 6 & 0,06 & 0 \\
ball,front,green,small & 0 & 0 & 5 & 0,05 & 0 \\
\hline
Total & 140 & 100 & 10000 & 100 & 80,6 \\
\hline
\end{tabular}
\caption{Referring expressions produced by the algorithm for Figure~\ref{GRE3D7-stimulus}\label{results-algo-fig3}}
\end{center}
\end{table}

In order to put our results in perspective we compare them to the results obtained for several of the figures in the corpus using the probability of used inferred (column Learned \puse) and the probability of used directly extracted from corpora (column Extracted \puse) as explained in Section~\ref{sec:learning}. We obtained the intersection with the corpora shown in Table~\ref{results-algo-all}. 

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Figure & Random & Learned \puse & Extracted \puse \\
\hline
Fig 1 & 5.38\% & 80.60\% & 84.27\% \\
Fig 3 & 3.29\% & 80.74\% & 83.19\% \\
Fig 6 & 12.76\%	& 84.00\% & 90.71\% \\
Fig 8 & 15.19\%	& 73.62\% & 85.14\% \\
Fig 10 & 7.94\%	& 60.85\% & 81.77\% \\
Fig 12 & 37.94\% & 84.15\% & 75.00\% \\
Fig 13 & 3.48\%	& 53.81\% & 93.57\% \\
Fig 21 & 7.92\%	& 82.00\% & 92.55\% \\
\hline
Average	& 11.74\% & 74.97\% & 85.77\% \\
\hline
\end{tabular}
\caption{Percentage on intersection between the REs generated using random probabilities, learned probabilities and probabilities directly extracted from the figure corpora\label{results-algo-all}}
\end{center}
\end{table}

The random baseline, whose results are shown in Table~\ref{results-algo-all} (column Random) is calculated in by producing random probabilities of use and then running the algorithm 10000 using these random probabilities. Not only the intersection between the randomly generated REs and the corpus is lower but also many of  the REs generated in this way are not naturally sounding (e.g. ``small on the top of a blue cube that is below of something that is small''). 
